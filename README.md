# humerus-bot

## Introduction

The humerus bot is a NLP project designed to *win* Cards Against Humanity.
This repository contains code for the [humerus bot](http://humerusdecks.com/#new) as well as artefacts/code from prior unsuccessful approaches.


## Background Information

[Cards Against Humanity](https://cardsagainsthumanity.com/) is a popular humor-based party game where one attempts to select the funniest response (white card) for a given prompt (black card).
The person who plays the funniest response "wins" the black card, and the first person to accumulate a predetermined number of black cards wins the overall game.
### Client
An existing cards against humanity implementation, [massive decks](https://github.com/Lattyware/massivedecks) was [forked](https://github.com/UTMIST/humerusdecks) and modified for game data collection and to integrate with our model backend.

This is packaged and deployed with docker on AWS. 

We used GPT-2 to generate a couple CAH decks for use but they ultimately did not make enough sense for a reasonable game experience. 

### Server/Model

#### Causual Language Modelling Approach

Assuming that large causual language models e.g. GPT-2 can reasonably model text semantics and meanings (at least superficially), one may use them to determine a "funnyness score"


@ Alston, can you extend on this?

 To mention:
1. how it works
2. mention GLTR [source below] because we are basically implementing that
3. proposed weighted log sum for funnyness score
4. why it didn't work: resource constraints

#### Simple NN
With resource constraints in mind we then implemented a more lightweight model. 

1. Possible plays (white card + black card combination) are embedded with BERT and then PCA'd. 69 dimensions turned out to be a good number.
2. As pre-generating all embeddings for all plays would take up too much memory, it is assumed that the PCA matrix will not vary significantly for other plays and so is saved for on-the-fly embedding in deployment.
2. A simple two-layer dense predictive neural net is trained on collected game data. 
3. Networks is deployed via a simple flask API

We believe that BERT embeddings would be a good representation of text generated by the black card and white card combinations.
 However, BERT embeddings are 768 dimensional; since we have 500 white cards, 66 one slot black cards, and 12 two slots black cards, it makes a total more than 3 million combinations of these 768-dimensional embeddings.
 A neural network that takes in 768-dimensional features input is too big to be implemented on our server and storing them takes a lot of computational resources.
 For this reason, we turned to PCA to reduce the BERT embeddings into smaller dimension but equally rich text representations.
 Running PCA on 3 million embeddings is simply infeasible.
 Hence, we ran PCA on a dataset consists of 33000 combinations from the one-slot black cards and 33000 sample combinations from the two-slot black cards.
 Using the eigenvector matrix generated from this dataset, we generalize PCA to other combinations.

 ![pca_69_dim_image](./assets/fig1.png)
 We found that reducing the BERT embeddings into 69 dimension keeps 88.6 % of the original representation while being at just about the inflection point of the % variance captured vs num_dimensions graph which is desirable.

![angle_1](./assets/fig2_1.png)
![angle_2](./assets/fig2_2.png)
To double check that our PCA works, we plotted a 3-dimensional PCA where each color represents a black card as shown above.
 Note that 3-dimensional PCA only keeps 21.6 % of the original information but the clusters are already visible, showing that similar meaning sentences do tend to cluster.
Overall, the PCA approach looks promising and was therefore deployed in our final bot.



### Challenges 
- model design choices: considering deployment, cost, runtime, etc
- Architecture (front/backend)
- What makes something funny? 


## What's next 
- benchmark model


## File & Folder Structure

```
.
├── data 
│   ├── cah-base-en.deck.json5
│   ├── Creating Training Set.ipynb
│   ├── embed_sentences.py
│   ├── Explore_PCA_BERT.ipynb
│   ├── pca_cache_69_5050.pkl
│   ├── pca_cache_69.pkl
│   ├── PCA for Humerus 2 Cards and Reddit.ipynb
│   ├── processed_oneliners.csv
│   ├── reduce_dim.py
│   ├── r-Oneliners-scraped.csv
│   └── training_set_data_69_5050.pkl
├── humerusbot
│   ├── bert
│   │   ├── api.py
│   │   ├── data
│   │   ├── models
│   │   │   ├── m1.h5
│   │   │   ├── m2.h5
│   │   │   └── README.md
│   │   ├── notebooks
│   │   │   ├── Bert Embeddings.ipynb
│   │   │   ├── humerus_prediction.ipynb
│   │   │   └── wordembed_to_bert_translation.ipynb
│   │   ├── requirements.txt
│   │   ├── train_script.py
│   │   └── utils
│   │       ├── dataManager.py
│   │       ├── humerus.py
│   │       ├── __init__.py
│   │       ├── pcaifier.py
│   │       └── requirements.txt
│   ├── gpt2
│   │   ├── finetune_gpt2.py
│   │   ├── Humerus_Finetuning.ipynb
│   │   ├── humerus_gpt2.py
│   │   ├── return_log_prob.py
│   │   └── Training_Huggingface.ipynb
│   └── utils
│       ├── CAH-Gen.ipynb
│       ├── cardutils.py
│       ├── classpca.py
│       └── GPT2-CAH-Generation.ipynb
├── README.md
└── requirements.txt

```

## References

https://cpury.github.io/ai-against-humanity/

GLTR: http://gltr.io/
GLTR Paper: https://arxiv.org/abs/1906.04043


### Team Members

```json5
{
        'Alston Lo' : "https://github.com/alstonlo",
        'Brian Chen' : "https://github.com/ihasdapie",
        'Matthew Ao' : "https://github.com/aoruize",
        'Jack Cai' : "https://github.com/caixunshiren",
        'Zoha Rehan' : "https://github.com/zoharehan"
}

```




































